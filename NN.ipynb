{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85994f81-1ed9-487c-aa34-1bf4626035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d15642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d0d2a2-0d31-488c-92eb-55fa1e8c0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read CSV without headers\n",
    "#df = pd.read_csv(\"TwoLeadECG_TEST.csv\", header=None)\n",
    "#df = pd.read_csv(\"TwoLeadECG_TRAIN.csv\", header=None)\n",
    "\n",
    "## Read .txt without headers and seperated by commas\n",
    "#df = pd.read_csv(\"Phi1.txt\", header=None)  # Default is comma separator\n",
    "df = pd.read_csv(\"Phi2.txt\", header=None)  # Default is comma separator\n",
    "\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Separate input features (columns 1 to end) and target (column 0)\n",
    "x = torch.tensor(df.iloc[:, 1:num_columns].values, dtype=torch.float32)  # Inputs: columns 1â€“9\n",
    "y = torch.tensor(df.iloc[:, 0].values, dtype=torch.float32).view(-1, 1)  # Target: column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f445662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network with one hidden layer\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        #self.activation = nn.LeakyReLU()\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02582bbd-4da4-4218-9f9a-2e53fd695397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom uniform norm (Chebyshev) loss function\n",
    "def chebyshev_loss(output, target):\n",
    "    return torch.max(torch.abs(output - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24470657-1879-4599-9d4c-9b00fe11f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 norm\n",
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780ff790-2878-48db-b7b3-e21930460b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.361376\n",
      "Epoch 1, Loss: 1.342299\n",
      "Epoch 2, Loss: 1.323335\n",
      "Epoch 3, Loss: 1.304441\n",
      "Epoch 4, Loss: 1.285636\n",
      "Epoch 5, Loss: 1.266945\n",
      "Epoch 6, Loss: 1.248409\n",
      "Epoch 7, Loss: 1.230033\n",
      "Epoch 8, Loss: 1.211840\n",
      "Epoch 9, Loss: 1.193788\n",
      "Epoch 10, Loss: 1.175865\n",
      "Epoch 11, Loss: 1.158038\n",
      "Epoch 12, Loss: 1.140391\n",
      "Epoch 13, Loss: 1.122938\n",
      "Epoch 14, Loss: 1.105744\n",
      "Epoch 15, Loss: 1.088693\n",
      "Epoch 16, Loss: 1.071814\n",
      "Epoch 17, Loss: 1.055245\n",
      "Epoch 18, Loss: 1.039022\n",
      "Epoch 19, Loss: 1.023261\n",
      "Epoch 20, Loss: 1.007925\n",
      "Epoch 21, Loss: 0.993077\n",
      "Epoch 22, Loss: 0.978675\n",
      "Epoch 23, Loss: 0.964744\n",
      "Epoch 24, Loss: 0.951218\n",
      "Epoch 25, Loss: 0.938069\n",
      "Epoch 26, Loss: 0.925459\n",
      "Epoch 27, Loss: 0.913180\n",
      "Epoch 28, Loss: 0.901314\n",
      "Epoch 29, Loss: 0.889810\n",
      "Epoch 30, Loss: 0.878820\n",
      "Epoch 31, Loss: 0.868223\n",
      "Epoch 32, Loss: 0.858028\n",
      "Epoch 33, Loss: 0.848305\n",
      "Epoch 34, Loss: 0.839078\n",
      "Epoch 35, Loss: 0.830224\n",
      "Epoch 36, Loss: 0.821711\n",
      "Epoch 37, Loss: 0.813631\n",
      "Epoch 38, Loss: 0.806062\n",
      "Epoch 39, Loss: 0.798911\n",
      "Epoch 40, Loss: 0.792114\n",
      "Epoch 41, Loss: 0.785755\n",
      "Epoch 42, Loss: 0.779814\n",
      "Epoch 43, Loss: 0.774277\n",
      "Epoch 44, Loss: 0.769054\n",
      "Epoch 45, Loss: 0.764183\n",
      "Epoch 46, Loss: 0.759733\n",
      "Epoch 47, Loss: 0.755582\n",
      "Epoch 48, Loss: 0.751687\n",
      "Epoch 49, Loss: 0.748019\n",
      "Epoch 50, Loss: 0.744610\n",
      "Epoch 51, Loss: 0.741422\n",
      "Epoch 52, Loss: 0.738373\n",
      "Epoch 53, Loss: 0.735464\n",
      "Epoch 54, Loss: 0.732642\n",
      "Epoch 55, Loss: 0.729924\n",
      "Epoch 56, Loss: 0.727277\n",
      "Epoch 57, Loss: 0.724709\n",
      "Epoch 58, Loss: 0.722244\n",
      "Epoch 59, Loss: 0.719820\n",
      "Epoch 60, Loss: 0.717400\n",
      "Epoch 61, Loss: 0.715011\n",
      "Epoch 62, Loss: 0.712650\n",
      "Epoch 63, Loss: 0.710331\n",
      "Epoch 64, Loss: 0.708025\n",
      "Epoch 65, Loss: 0.705755\n",
      "Epoch 66, Loss: 0.703531\n",
      "Epoch 67, Loss: 0.701362\n",
      "Epoch 68, Loss: 0.699205\n",
      "Epoch 69, Loss: 0.697079\n",
      "Epoch 70, Loss: 0.695032\n",
      "Epoch 71, Loss: 0.693081\n",
      "Epoch 72, Loss: 0.691186\n",
      "Epoch 73, Loss: 0.689336\n",
      "Epoch 74, Loss: 0.687551\n",
      "Epoch 75, Loss: 0.685804\n",
      "Epoch 76, Loss: 0.684091\n",
      "Epoch 77, Loss: 0.682431\n",
      "Epoch 78, Loss: 0.680817\n",
      "Epoch 79, Loss: 0.679251\n",
      "Epoch 80, Loss: 0.677733\n",
      "Epoch 81, Loss: 0.676270\n",
      "Epoch 82, Loss: 0.674856\n",
      "Epoch 83, Loss: 0.673498\n",
      "Epoch 84, Loss: 0.672174\n",
      "Epoch 85, Loss: 0.670875\n",
      "Epoch 86, Loss: 0.669610\n",
      "Epoch 87, Loss: 0.668386\n",
      "Epoch 88, Loss: 0.667194\n",
      "Epoch 89, Loss: 0.666026\n",
      "Epoch 90, Loss: 0.664872\n",
      "Epoch 91, Loss: 0.663738\n",
      "Epoch 92, Loss: 0.662628\n",
      "Epoch 93, Loss: 0.661526\n",
      "Epoch 94, Loss: 0.660430\n",
      "Epoch 95, Loss: 0.659340\n",
      "Epoch 96, Loss: 0.658261\n",
      "Epoch 97, Loss: 0.657189\n",
      "Epoch 98, Loss: 0.656112\n",
      "Epoch 99, Loss: 0.655060\n",
      "\n",
      " Total training time: 0.0592 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy data\n",
    "    # x = torch.tensor([[0.1], [0.5], [0.9]], dtype=torch.float32)  # 3 samples, 1 feature\n",
    "    # y = torch.tensor([[0.2], [0.4], [0.8]], dtype=torch.float32)  # target values\n",
    "\n",
    "    # Network setup\n",
    "    net = SimpleNet(input_size=num_columns-1, hidden_size=2, output_size=1)\n",
    "    #optimizer = torch.optim.Adamax(net.parameters(), lr=0.01)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(x)\n",
    "        #loss = chebyshev_loss(output, y)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n Total training time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5557528-7c24-4720-8048-cc1ed496b7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signs at last epoch (per sample, per hidden node):\n",
      "tensor([[-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ADD THE SIGN-DISTRIBUTION CODE HERE (FINAL STEP)\n",
    "# ============================================================\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = net.hidden(x)        # shape (N, hidden_size)\n",
    "    signs = torch.sign(z)    # -1 or +1 (0 extremely unlikely)\n",
    "\n",
    "print(\"Signs at last epoch (per sample, per hidden node):\")\n",
    "print(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de0834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
